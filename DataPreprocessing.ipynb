{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark #only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Data Preprocessing').getOrCreate()\n",
    "spark\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.feature import StandardScaler, PCA\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = spark.read.csv('DailyDemandForecastingOrdersEditSJ.csv', header = True, inferSchema = True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shorten the features name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove null value records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.na.drop(how = 'all')\n",
    "\n",
    "# Check if there is any NULL values. If yes, remove them\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "df1.select([count(when(col(c).isNull(), c)).alias(c) for c in df1.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Imputer class from PySpark\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Create an instance of imputer for all missing values (numeric fields)\n",
    "myImputer = Imputer(\n",
    "                inputCols = ['UrgentOrd', 'FiscalSecOrd'],\n",
    "                outputCols = ['UrgentOrd', 'FiscalSecOrd']\n",
    ").setStrategy('mean')\n",
    "\n",
    "df1 = myImputer.fit(df1).transform(df1)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.corr('UrgentOrd','FiscalSecOrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df1.drop('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresRDD = features.rdd.map(lambda row: row[0:])\n",
    "featuresRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Statistics.colStats(featuresRDD)\n",
    "print(summary.mean())\n",
    "print(summary.variance())\n",
    "print(summary.numNonzeros())\n",
    "print(summary.normL1())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking correlation using pearson method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df1.drop('Target')\n",
    "assembler = VectorAssembler(inputCols=features.columns, outputCol='features')\n",
    "output = assembler.transform(df1)\n",
    "output.select('features', 'Target').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df1.select('Target')\n",
    "label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df1.drop('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = features.columns\n",
    "featuresRDD = features.rdd.map(lambda row: row[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(featuresRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledFeatures = scaler1.transform(featuresRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in scaledFeatures.collect():\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "Principal component analysis (PCA) is an unsupervised technique used to preprocess and reduce the dimensionality of high-dimensional datasets while preserving the original structure and relationships inherent to the original dataset so that machine learning models can still learn from them and be used to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=3)\n",
    "pcaModel = pca.fit(scaledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pcaModel.transform(scaledFeatures)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dense sector in a dataframe\n",
    "df3 = result.map(lambda x: (x, )).toDF(['PCAFeatures'])\n",
    "df3.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "af46dcb3ebbbe05d6be934d87b376fbeb35fe9dd388efbf850fb580e2f249917"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
